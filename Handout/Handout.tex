\documentclass{article}

\usepackage{amsmath}
\usepackage{fancyhdr} % Required for custom headers
\usepackage{lastpage} % Required to determine the last page

%----------------------------------------------------------------------------------------
% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

% Set up the header and footer
\pagestyle{fancy}
\lhead{Alex Pan and Alec Kosik} % Top left header
\chead{\textbf{Particle Filters}} % Top center head
\rhead{Page\ \thepage\ of\ \protect\pageref{LastPage}} % Top right header
\renewcommand\headrulewidth{0.4pt} % Size of the header rule

% Text formatting
\linespread{1.1} % Linespacing
% \setlength\parindent{0pt} % Removes all indentation from paragraphs

%----------------------------------------------------------------------------------------
\begin{document}


% --------------------- Preliminaries ---------------------
\section{Hidden Markov Models}

A \textbf{Hidden Markov Model} (HMM) is defined by an underlying system that is a Markov process with states that cannot be observed, but gives outputs that are dependent on the hidden state of the system. If we let random variable $X_i$ represent the $i$-th state of the system, and $Y_i$ represent the $i$-th output of the system, we have:

\begin{equation}
X_1 \sim \mu (x_1) \textnormal{ and } X_n|(X_{n-1}=x_{n-1}) \sim f(x_n|x_{n-1})
\end{equation}
\begin{equation}
Y_n|(X_n = x_n) \sim g(y_n|x_n)
\end{equation}

where $X_1$ is defined by some initial conditions $\mu(x_1)$.

% --------------------- Setting up the problem ---------------------
\section{Particle Filters}

Basically, the big idea is to use our observations of the system to determine the hidden state of the system. That is to say, we want to figure out $p(x_{n}|y_{1:n})$. Recognizing that this is a marginal is useful because then we can just say:

\begin{equation}
p(x_{n}|y_{1:n}) = \int p(x_{1:n}|y_{1:n}) dx_{1:n-1}
\end{equation}

Setting this as a marginal of $p(x_{1:n}|y_{1:n})$ is helpful because we can refactor this conditional distribution to see how it comes from our prior distributions of the hidden state and observations. With some refactoring and using our initial HMM conditions we can show:

\begin{equation}
\begin{split}
% p(x_{1:n}|y_{1:n}) &= \frac{p(x_{1:n},y_{1:n})}{p(y_{1:n})}\\
% p(x_{1:n},y_{1:n}) &= p(x_{1:n}) p(y_{1:n}|x_{1:n})\\
% p(x_{1:n}) &= \mu(x_1)\prod_{i=2}^{n} f(x_1|x_{i-1})\\
% p(y_{1:n}|x_{1:n}) &= \prod_{i=1}^{n} g(y_i|x_i)\\
% p(y_{1:n}) &= \int p(x_{1:n},y_{1:n}) dx_{1:n}\\
p(x_{n}|y_{1:n}) &= \int \frac{\mu(x_1)\prod_{i=2}^{n} f(x_1|x_{i-1})\prod_{i=1}^{n} g(y_i|x_i)}{\int \mu(x_1)\prod_{i=2}^{n} f(x_1|x_{i-1})\prod_{i=1}^{n} g(y_i|x_i) dx_{1:n}} dx_{1:n-1}
\end{split}
\end{equation}

This looks really messy, but it's helpful to see that this is just integration of functions we already know $f$ and $g$. In other words, we already have all the information we need to arrive at the answer. 

The issue now is that this is sometimes intractable. If functions are nice, like guassians or linear distributions, this is integrable and we can arrive at a solution analytically. However, this is not always the case, which is why we sometimes need to derive an answer from numeric approximations.

% --------------------- Kalman Filter ---------------------
\section{Kalman Filter}

So let's say you're really lucky and your initial distribution is something nice. This allows us to get a solution analytically through a method called the Kalman Filter. We can reformulate our goal as:

\begin{equation}
\textnormal{Prediction step:  }
p(x_{n}|y_{1:n}) = \int f(x_n|x_{1:n-1})p(x_{n-1}|y_{1:n-1}) dx_{n-1}
\end{equation}

\begin{equation}
\textnormal{Update step:  }
p(x_{n}|y_{1:n}) \propto g(y_n|x_n)p(x_{n-1}|y_{1:n-1})
\end{equation}

Essentially, we are now assuming that $p(x_{n-1}|y_{1:n-1})$ is known due to recursion. To see how this relates to our earlier definition of $p(x_{n}|y_{1:n})$ in Eq. (4), note that this recursion is what gives us the numerator, while the normalization we do in the update step essentially gives us the denominator. 

% --------------------- SMC Methods ---------------------
\section{Sequential Monte Carlo Methods}

So what if our distribution isn't nice? To find the our desired distribution we will use an approximation:

\begin{equation}
\textnormal{Prediction step:  }
\pi_{x_{1:t}|y_{1:t-1}}(x_{1:t}|y_{1:t-1})= f(x_t|x_{t-1}) \pi_{x_{1:t-1}|y_{1:t-1}}(x_{1:t-1}|y_{1:t-1})
\end{equation}

\begin{equation}
\textnormal{Update step:  }
\pi_{x_{1:t}|y_{1:t}}(x_{1:t}|y_{1:t})\propto g(y_t|x_t)\pi_{x_{1:t}|y_{1:t-1}}(x_{1:t}|y_{1:t-1})
\end{equation}

% --------------------- SIS ---------------------
\section{SIS}
\begin{enumerate}
\item \textit{Initialization:}
\item[] For $i=1,\dots N$:
\begin{itemize}
\item[] Sample $x_0^{(i)} \sim \mu(x_0)$ 
\item[] Assign weights $\widetilde{w}_0^{(i)} = g(y_0|x_0^{(i)})$
\item[] Normalize weights $w_0^{(i)} = \frac{\widetilde{w}_0^{(i)}}{\sum_{i=1}^{n} \widetilde{w}_0^{(i)}}$
\end{itemize}
\item \textit{Importance Sampling:}
\item[] For $t=1,\dots,T$:
\begin{itemize}
\item[] Sample $x_t^{(i)} \sim f(x_t|x_{t-1}^{i})$
% w_{t-1}^{(i)} 
\item[] Assign weights $\widetilde{w}_t^{(i)} = g(y_t|x_t^{(i)})$
\item[] Normalize weights $w_t^{(i)} = \frac{\widetilde{w}_t^{(i)}}{\sum_{i=1}^{n} \widetilde{w}_t^{(i)}}$
\end{itemize}
\item Return $\{x_T^{(i)},w_T^{(i)}\}_{i=1}^N$
\end{enumerate}

We never use this is real life beacuse of the degeneracy problem.

% --------------------- Degeneracy Problem ---------------------
\section{Degeneracy Problem}
When we are sampling from our reweighted distribution, points with more mass tend to get more mass, while points with fewer mass tend to get less mass. Over a large number of iterations, this manifests as a handful of points having nearly all the weight while the rest of our points become negligable. 


% --------------------- Bootstrap/SIR ---------------------
\section{Bootstrap (SIR)}

We resample to deal with the degeneracy problem. Modifying step 2 gives us:

\begin{enumerate}
\setcounter{enumi}{1}
\item \textit{Importance Sampling and Resampling:}
\item[] For $t=1,\dots,T$:
\begin{itemize}
\item[] Sample $x_t^{(i)} \sim f(x_t|\widetilde{x}_{t-1}^{i})$
\item[] Assign weights $\widetilde{w}_t^{(i)} = g(y_t|x_t^{(i)})$
\item[] Normalize weights $w_t^{(i)} = \frac{\widetilde{w}_t^{(i)}}{\sum_{i=1}^{n} \widetilde{w}_t^{(i)}}$
\item[] Resample $\widetilde{x}_t^{(i)}$ from $x_t^{(i)}$ according to the weight distribution.
\end{itemize}
\end{enumerate}


Except that there's a whole new problem called the sampling impoverishment problem. There are methods that address both of these issues but they require fancy sounding things like the ``Epanechnikov Kernel''.



% --------------------- Some equations!!! ---------------------

% \section{Lots of equations we'll use:}
% \begin{equation}
% \textnormal{Prior: }
% p(x_{1:n}) = \mu(x_1)\prod_{i=2}^{n} f(x_1|x_{i-1})
% \end{equation}

% \begin{equation}
% \textnormal{Conditional of Y: }
% p(y_{1:n}|x_{1:n}) = \prod_{i=1}^{n} g(y_i|x_i)
% \end{equation}

% \begin{equation}
% \textnormal{Conditional of X: }
% p(x_{1:n}|y_{1:n}) = \frac{p(x_{1:n},y_{1:n})}{p(y_{1:n})}
% \end{equation}

% \begin{equation}
% \textnormal{Joint: }
% p(x_{1:n},y_{1:n}) = p(x_{1:n}) p(y_{1:n}|x_{1:n})
% \end{equation}

% \begin{equation}
% \textnormal{Marginal: }
% p(y_{1:n}) = \int p(x_{1:n},y_{1:n}) dx_{1:n}
% \end{equation}

% \begin{equation}
% \textnormal{We have: }
% \{p(x_{1:n}|y_{1:n})\}_{n>0}
% \textnormal{ and }
% \{p(y_{1:n})\}_{n>0}
% \end{equation}

% \begin{equation}
% \textnormal{Want to find: }
% \{p(x_n|y_{1:n})\}_{n>0}
% \end{equation}

% \begin{equation}
% \textnormal{Reformulate the joint: }
% p(x_{1:n},y_{1:n}) = p(x_{1:n-1},y_{1:n-1}) f(x_n|x_{n-1}) g(y_n|x_n)
% \end{equation}

% \begin{equation}
% \begin{split}
% p(x_{1:n},y_{1:n}) &= p(x_{1:n}) p(y_{1:n}|x_{1:n})\\
% p(x_{1:n}) &= \mu(x_1)\prod_{i=2}^{n} f(x_1|x_{i-1})
% = (\mu(x_1)\prod_{i=2}^{n-1} f(x_1|x_{i-1})) f(x_n|x_{n-1})
% = p(x_{1:n-1} (x_n|x_{n-1}))\\
% p(y_{1:n}|x_{1:n}) &= \prod_{i=1}^{n} g(y_i|x_i)
% = (\prod_{i=1}^{n-1} g(y_i|x_i)) g(y_n|x_n)
% = p(y_{1:n-1}|x_{1:n-1}) g(y_n|x_n)\\
% p(y_{1:n-1}|x_{1:n-1}) &= \frac{p(x_{1:n-1},y_{1:n-1})}{p(y_{1:n-1})}\\
% p(x_{1:n},y_{1:n}) &= p(x_{1:n-1} (x_n|x_{n-1})) \frac{p(x_{1:n-1},y_{1:n-1})}{p(y_{1:n-1})} g(y_n|x_n)
% = p(x_{1:n-1},y_{1:n-1}) f(x_n|x_{n-1}) g(y_n|x_n)
% \end{split}
% \end{equation}

% \begin{equation}
% \textnormal{Reformulate the conditional of X: }
% p(x_{1:n}|y_{1:n}) = p(x_{1:n-1}|y_{1:n-1})\frac{f(x_n|x_{n-1}) g(y_n|x_n)}{p(y_n|y_{1:n-1})}
% \end{equation}

% \begin{equation}
% \begin{split}
% p(x_{1:n}|y_{1:n}) &= \frac{p(x_{1:n},y_{1:n})}{p(y_{1:n})}\\
% p(x_{1:n},y_{1:n}) &= p(x_{1:n-1},y_{1:n-1}) f(x_n|x_{n-1}) g(y_n|x_n)\\
% p(x_{1:n-1},y_{1:n-1}) &= p(y_{1:n-1}) p(x_{1:n-1}|y_{1:n-1})\\
% p(y_{1:n-1}) &= \frac{p(y_{1:n-1},y_n)}{p(y_n|y_{1:n-1})} = \frac{p(y_{1:n})}{p(y_n|y_{1:n-1})}\\
% p(x_{1:n}|y_{1:n}) &= \frac{1}{p(y_{1:n})}\frac{p(y_{1:n})}{p(y_n|y_{1:n-1})} p(x_{1:n-1}|y_{1:n-1}) f(x_n|x_{n-1}) g(y_n|x_n)\\
% &= p(x_{1:n-1}|y_{1:n-1})\frac{f(x_n|x_{n-1}) g(y_n|x_n)}{p(y_n|y_{1:n-1})}\\
% \end{split}
% \end{equation}

% \begin{equation}
% \pi_n(x_{1:n})=p(x_{1:n}|y_{1:n}) = \frac{p(x_{1:n},y_{1:n})}{p(y_{1:n})}
% \end{equation}

% \begin{equation}
% \hat{\pi}_n(x_{1:n})=\frac{1}{N}\sum_{i=1}^{N}\delta_{X_{1:n}^i}(x_{1:n})
% \end{equation}

% \begin{equation}
% \hat{\pi}_n(x_k)=\frac{1}{N}\sum_{i=1}^{N}\delta_{X_k^i}(x_k)
% \end{equation}

\end{document}