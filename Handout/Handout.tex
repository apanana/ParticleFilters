\documentclass{article}

\usepackage{amsmath}
\usepackage{fancyhdr} % Required for custom headers
\usepackage{lastpage} % Required to determine the last page

%----------------------------------------------------------------------------------------
% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

% Set up the header and footer
\pagestyle{fancy}
\lhead{Alex Pan and Alec Kosik} % Top left header
\chead{\textbf{Particle Filters}} % Top center head
\rhead{Page\ \thepage\ of\ \protect\pageref{LastPage}} % Top right header
\renewcommand\headrulewidth{0.4pt} % Size of the header rule

% Text formatting
\linespread{1.1} % Linespacing
% \setlength\parindent{0pt} % Removes all indentation from paragraphs

%----------------------------------------------------------------------------------------
\begin{document}


% --------------------- Preliminaries ---------------------
\section{Hidden Markov Models}

A \textbf{Hidden Markov Model} (HMM) is defined by an underlying system that is a Markov process with states that cannot be observed, but gives outputs that are dependent on the hidden state of the system. If we let random variable $X_i$ represent the $i$-th state of the system, and $Y_i$ represent the $i$-th output of the system, we have:

\begin{equation}
X_1 \sim \mu (x_1) \textnormal{ and } X_n|(X_{n-1}=x_{n-1}) \sim f(x_n|x_{n-1})
\end{equation}
\begin{equation}
Y_n|(X_n = x_n) \sim g(y_n|x_n)
\end{equation}

where $X_1$ is defined by some initial conditions $\mu(x_1)$.

% --------------------- Setting up the problem ---------------------
\section{Particle Filters}

Basically, the big idea is to use our observations of the system to determine the hidden state of the system. That is to say, we want to figure out $p(x_{n}|y_{1:n})$. Recognizing that this is a marginal is useful because then we can just say:

\begin{equation}
p(x_{n}|y_{1:n}) = \int p(x_{1:n}|y_{1:n}) dx_{1:n-1}
\end{equation}

Setting this as a marginal of $p(x_{1:n}|y_{1:n})$ is helpful because we can refactor this conditional distribution to see how it comes from our prior distributions of the hidden state and observations.

\begin{equation}
\begin{split}
p(x_{1:n}|y_{1:n}) &= \frac{p(x_{1:n},y_{1:n})}{p(y_{1:n})}\\
p(x_{1:n},y_{1:n}) &= p(x_{1:n}) p(y_{1:n}|x_{1:n})\\
p(x_{1:n}) &= \mu(x_1)\prod_{i=2}^{n} f(x_1|x_{i-1})\\
p(y_{1:n}|x_{1:n}) &= \prod_{i=1}^{n} g(y_i|x_i)\\
p(y_{1:n}) &= \int p(x_{1:n},y_{1:n}) dx_{1:n}\\
p(x_{n}|y_{1:n}) &= \int \frac{\mu(x_1)\prod_{i=2}^{n} f(x_1|x_{i-1})\prod_{i=1}^{n} g(y_i|x_i)}{\int \mu(x_1)\prod_{i=2}^{n} f(x_1|x_{i-1})\prod_{i=1}^{n} g(y_i|x_i) dx_{1:n}} dx_{1:n-1}
\end{split}
\end{equation}

This looks really messy, but it's helpful to see that this is just integration of functions we already know $f$ and $g$. In other words, we already have all the information we need to arrive at the answer. 

The issue now is that this is sometimes intractable. If functions are nice, like guassians or linear distributions, this is integrable and we can arrive at a solution analytically. However, this is not always the case, which is why we need to derive an answer from numeric approximations.

% --------------------- Kalman Filter ---------------------
\section{Kalman Filter}

So let's say you're really lucky and your initial distribution is something nice. This allows us to get a solution analytically through a method called the Kalman Filter. We can reformulate our goal as:

\begin{equation}
\textnormal{Prediction step:  }
p(x_{n}|y_{1:n}) = \int f(x_n|x_{1:n-1})p(x_{n-1}|y_{1:n-1}) dx_{n-1}
\end{equation}

We assume that $p(x_{n-1}|y_{1:n-1})$ is known due to recursion. 

\begin{equation}
\textnormal{Update step:  }
p(x_{n}|y_{1:n}) = \propto g(y_n|x_n)p(x_n|y_{1:n-1})
\end{equation}


% --------------------- SMC Methods ---------------------
\section{Sequential Monte Carlo Methods}

So what if our distribution isn't nice? Rekt.

% --------------------- SIS ---------------------
\section{SIS}


% --------------------- Bootstrap/SIR ---------------------
\section{Bootstrap (SIR)}

We resample to deal with the degeneracy problem.

Except that there's a whole new problem called the sampling impoverishment problem. There are methods that address both of these issues but they require fancy sounding things like the ``Epanechnikov Kernel''.



% --------------------- Some equations!!! ---------------------

% \section{Lots of equations we'll use:}
% \begin{equation}
% \textnormal{Prior: }
% p(x_{1:n}) = \mu(x_1)\prod_{i=2}^{n} f(x_1|x_{i-1})
% \end{equation}

% \begin{equation}
% \textnormal{Conditional of Y: }
% p(y_{1:n}|x_{1:n}) = \prod_{i=1}^{n} g(y_i|x_i)
% \end{equation}

% \begin{equation}
% \textnormal{Conditional of X: }
% p(x_{1:n}|y_{1:n}) = \frac{p(x_{1:n},y_{1:n})}{p(y_{1:n})}
% \end{equation}

% \begin{equation}
% \textnormal{Joint: }
% p(x_{1:n},y_{1:n}) = p(x_{1:n}) p(y_{1:n}|x_{1:n})
% \end{equation}

% \begin{equation}
% \textnormal{Marginal: }
% p(y_{1:n}) = \int p(x_{1:n},y_{1:n}) dx_{1:n}
% \end{equation}

% \begin{equation}
% \textnormal{We have: }
% \{p(x_{1:n}|y_{1:n})\}_{n>0}
% \textnormal{ and }
% \{p(y_{1:n})\}_{n>0}
% \end{equation}

% \begin{equation}
% \textnormal{Want to find: }
% \{p(x_n|y_{1:n})\}_{n>0}
% \end{equation}

% \begin{equation}
% \textnormal{Reformulate the joint: }
% p(x_{1:n},y_{1:n}) = p(x_{1:n-1},y_{1:n-1}) f(x_n|x_{n-1}) g(y_n|x_n)
% \end{equation}

% \begin{equation}
% \begin{split}
% p(x_{1:n},y_{1:n}) &= p(x_{1:n}) p(y_{1:n}|x_{1:n})\\
% p(x_{1:n}) &= \mu(x_1)\prod_{i=2}^{n} f(x_1|x_{i-1})
% = (\mu(x_1)\prod_{i=2}^{n-1} f(x_1|x_{i-1})) f(x_n|x_{n-1})
% = p(x_{1:n-1} (x_n|x_{n-1}))\\
% p(y_{1:n}|x_{1:n}) &= \prod_{i=1}^{n} g(y_i|x_i)
% = (\prod_{i=1}^{n-1} g(y_i|x_i)) g(y_n|x_n)
% = p(y_{1:n-1}|x_{1:n-1}) g(y_n|x_n)\\
% p(y_{1:n-1}|x_{1:n-1}) &= \frac{p(x_{1:n-1},y_{1:n-1})}{p(y_{1:n-1})}\\
% p(x_{1:n},y_{1:n}) &= p(x_{1:n-1} (x_n|x_{n-1})) \frac{p(x_{1:n-1},y_{1:n-1})}{p(y_{1:n-1})} g(y_n|x_n)
% = p(x_{1:n-1},y_{1:n-1}) f(x_n|x_{n-1}) g(y_n|x_n)
% \end{split}
% \end{equation}

% \begin{equation}
% \textnormal{Reformulate the conditional of X: }
% p(x_{1:n}|y_{1:n}) = p(x_{1:n-1}|y_{1:n-1})\frac{f(x_n|x_{n-1}) g(y_n|x_n)}{p(y_n|y_{1:n-1})}
% \end{equation}

% \begin{equation}
% \begin{split}
% p(x_{1:n}|y_{1:n}) &= \frac{p(x_{1:n},y_{1:n})}{p(y_{1:n})}\\
% p(x_{1:n},y_{1:n}) &= p(x_{1:n-1},y_{1:n-1}) f(x_n|x_{n-1}) g(y_n|x_n)\\
% p(x_{1:n-1},y_{1:n-1}) &= p(y_{1:n-1}) p(x_{1:n-1}|y_{1:n-1})\\
% p(y_{1:n-1}) &= \frac{p(y_{1:n-1},y_n)}{p(y_n|y_{1:n-1})} = \frac{p(y_{1:n})}{p(y_n|y_{1:n-1})}\\
% p(x_{1:n}|y_{1:n}) &= \frac{1}{p(y_{1:n})}\frac{p(y_{1:n})}{p(y_n|y_{1:n-1})} p(x_{1:n-1}|y_{1:n-1}) f(x_n|x_{n-1}) g(y_n|x_n)\\
% &= p(x_{1:n-1}|y_{1:n-1})\frac{f(x_n|x_{n-1}) g(y_n|x_n)}{p(y_n|y_{1:n-1})}\\
% \end{split}
% \end{equation}

% \begin{equation}
% \pi_n(x_{1:n})=p(x_{1:n}|y_{1:n}) = \frac{p(x_{1:n},y_{1:n})}{p(y_{1:n})}
% \end{equation}

% \begin{equation}
% \hat{\pi}_n(x_{1:n})=\frac{1}{N}\sum_{i=1}^{N}\delta_{X_{1:n}^i}(x_{1:n})
% \end{equation}

% \begin{equation}
% \hat{\pi}_n(x_k)=\frac{1}{N}\sum_{i=1}^{N}\delta_{X_k^i}(x_k)
% \end{equation}

\end{document}